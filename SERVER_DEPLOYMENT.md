# æœåŠ¡å™¨éƒ¨ç½²æŒ‡å—

æœ¬æ–‡æ¡£å°†æŒ‡å¯¼æ‚¨å¦‚ä½•åœ¨è‡ªå·±çš„æœåŠ¡å™¨ä¸Šéƒ¨ç½²ä¼˜è´¨ç§‘æŠ€ä¿¡æ¯çˆ¬å–ç³»ç»Ÿã€‚

## ğŸ“‹ å‰ç½®è¦æ±‚

### æœåŠ¡å™¨ç¯å¢ƒ
- **æ“ä½œç³»ç»Ÿ**: Linux (Ubuntu 20.04+, CentOS 7+, Debian 10+ç­‰)
- **Python**: 3.10æˆ–æ›´é«˜ç‰ˆæœ¬
- **å†…å­˜**: è‡³å°‘512MB
- **ç£ç›˜**: è‡³å°‘1GBå¯ç”¨ç©ºé—´
- **ç½‘ç»œ**: èƒ½å¤Ÿè®¿é—®å¤–ç½‘(RSSæºã€Discordã€OpenAI API)

### å¿…éœ€çš„é…ç½®
- Discord Webhook URL
- OpenAI API Key (å¯é€‰,ç”¨äºAIç¿»è¯‘å’Œæ€»ç»“)

## ğŸš€ å¿«é€Ÿéƒ¨ç½²

### æ­¥éª¤1: å…‹éš†ä»“åº“

```bash
# SSHæ–¹å¼
git clone git@github.com:Xiangyu-Li97/tech-info-crawler.git

# æˆ–HTTPSæ–¹å¼
git clone https://github.com/Xiangyu-Li97/tech-info-crawler.git

# è¿›å…¥é¡¹ç›®ç›®å½•
cd tech-info-crawler
```

### æ­¥éª¤2: å®‰è£…ä¾èµ–

```bash
# å®‰è£…Pythonä¾èµ–
pip3 install -r requirements.txt

# æˆ–ä½¿ç”¨è™šæ‹Ÿç¯å¢ƒ(æ¨è)
python3 -m venv venv
source venv/bin/activate
pip install -r requirements.txt
```

### æ­¥éª¤3: é…ç½®ç³»ç»Ÿ

```bash
# å¤åˆ¶é…ç½®æ–‡ä»¶ç¤ºä¾‹
cp config.env.example config.env

# ç¼–è¾‘é…ç½®æ–‡ä»¶
nano config.env  # æˆ–ä½¿ç”¨ vim, vi ç­‰ç¼–è¾‘å™¨
```

åœ¨ `config.env` ä¸­å¡«å†™:

```bash
# Discord Webhook URL (å¿…éœ€)
DISCORD_WEBHOOK_URL="https://discord.com/api/webhooks/YOUR_WEBHOOK_ID/YOUR_WEBHOOK_TOKEN"

# OpenAI API Key (å¯é€‰)
OPENAI_API_KEY="sk-your-openai-api-key-here"
```

### æ­¥éª¤4: æµ‹è¯•è¿è¡Œ

```bash
# æ‰‹åŠ¨æ‰§è¡Œä¸€æ¬¡å®Œæ•´æµç¨‹
./run_daily.sh
```

æ‰§è¡ŒæˆåŠŸå:
- æŸ¥çœ‹æ—¥å¿—: `tail -f logs/crawler_YYYYMMDD.log`
- æ£€æŸ¥Discordé¢‘é“æ˜¯å¦æ”¶åˆ°æ¶ˆæ¯
- ç¡®è®¤æ•°æ®æ–‡ä»¶å·²ç”Ÿæˆ: `ls -lh processed_data_*.json`

### æ­¥éª¤5: é…ç½®å®šæ—¶ä»»åŠ¡

ä½¿ç”¨croné…ç½®æ¯å¤©è‡ªåŠ¨æ‰§è¡Œ:

```bash
# ç¼–è¾‘crontab
crontab -e

# æ·»åŠ ä»¥ä¸‹è¡Œ(æ¯å¤©æ—©ä¸Š9:00æ‰§è¡Œ)
0 9 * * * /path/to/tech-info-crawler/run_daily.sh

# ä¿å­˜å¹¶é€€å‡º
```

**é‡è¦**: å°† `/path/to/tech-info-crawler` æ›¿æ¢ä¸ºå®é™…çš„é¡¹ç›®è·¯å¾„ã€‚

æŸ¥çœ‹å®Œæ•´è·¯å¾„:
```bash
cd tech-info-crawler
pwd
```

### æ­¥éª¤6: éªŒè¯å®šæ—¶ä»»åŠ¡

```bash
# æŸ¥çœ‹å·²é…ç½®çš„å®šæ—¶ä»»åŠ¡
crontab -l

# æŸ¥çœ‹cronæ—¥å¿—(Ubuntu/Debian)
grep CRON /var/log/syslog

# æŸ¥çœ‹cronæ—¥å¿—(CentOS/RHEL)
grep CRON /var/log/cron
```

## ğŸ“Š Cronæ—¶é—´é…ç½®

### å¸¸ç”¨æ—¶é—´ç¤ºä¾‹

```bash
# æ¯å¤©æ—©ä¸Š9:00
0 9 * * *

# æ¯å¤©æ™šä¸Š21:00
0 21 * * *

# æ¯å¤©æ—©ä¸Š9:00å’Œæ™šä¸Š21:00
0 9,21 * * *

# æ¯6å°æ—¶æ‰§è¡Œä¸€æ¬¡
0 */6 * * *

# æ¯å‘¨ä¸€æ—©ä¸Š9:00
0 9 * * 1

# å·¥ä½œæ—¥(å‘¨ä¸€åˆ°å‘¨äº”)æ—©ä¸Š9:00
0 9 * * 1-5
```

### Cronè¡¨è¾¾å¼æ ¼å¼

```
åˆ†é’Ÿ(0-59) å°æ—¶(0-23) æ—¥(1-31) æœˆ(1-12) æ˜ŸæœŸ(0-6,0=å‘¨æ—¥)
```

## ğŸ”§ é«˜çº§é…ç½®

### ä½¿ç”¨è™šæ‹Ÿç¯å¢ƒ

æ¨èä½¿ç”¨Pythonè™šæ‹Ÿç¯å¢ƒ,é¿å…ä¾èµ–å†²çª:

```bash
# åˆ›å»ºè™šæ‹Ÿç¯å¢ƒ
python3 -m venv venv

# æ¿€æ´»è™šæ‹Ÿç¯å¢ƒ
source venv/bin/activate

# å®‰è£…ä¾èµ–
pip install -r requirements.txt

# è™šæ‹Ÿç¯å¢ƒä¼šè¢« run_daily.sh è‡ªåŠ¨æ£€æµ‹å’Œæ¿€æ´»
```

### é…ç½®ç¯å¢ƒå˜é‡

å¦‚æœä¸æƒ³ä½¿ç”¨ `config.env` æ–‡ä»¶,å¯ä»¥åœ¨ç³»ç»Ÿç¯å¢ƒå˜é‡ä¸­é…ç½®:

```bash
# ç¼–è¾‘ ~/.bashrc æˆ– ~/.profile
export DISCORD_WEBHOOK_URL="https://discord.com/api/webhooks/..."
export OPENAI_API_KEY="sk-..."

# é‡æ–°åŠ è½½é…ç½®
source ~/.bashrc
```

### ä¿®æ”¹æ¨é€æ–‡ç« æ•°é‡

ç¼–è¾‘ `discord_sender.py`:

```python
# æ‰¾åˆ°è¿™ä¸€è¡Œå¹¶ä¿®æ”¹ top_n å‚æ•°
send_daily_report_to_discord(webhook_url, top_n=20)  # æ”¹ä¸º20ç¯‡
```

### ç¦ç”¨AIå¤„ç†(èŠ‚çœæˆæœ¬)

å¦‚æœä¸éœ€è¦AIç¿»è¯‘å’Œæ€»ç»“:

1. ä¸é…ç½® `OPENAI_API_KEY`
2. æˆ–åœ¨ `run_daily.sh` ä¸­å¼ºåˆ¶ç¦ç”¨:
   ```bash
   python3 "$SCRIPT_DIR/data_processor.py" --no-ai
   ```

## ğŸ“ æ—¥å¿—ç®¡ç†

### æŸ¥çœ‹æ—¥å¿—

```bash
# æŸ¥çœ‹ä»Šå¤©çš„æ—¥å¿—
tail -f logs/crawler_$(date +%Y%m%d).log

# æŸ¥çœ‹æœ€è¿‘çš„æ—¥å¿—
ls -lt logs/ | head -10

# æœç´¢é”™è¯¯ä¿¡æ¯
grep "é”™è¯¯\|å¤±è´¥\|Error" logs/crawler_*.log
```

### æ—¥å¿—è‡ªåŠ¨æ¸…ç†

`run_daily.sh` ä¼šè‡ªåŠ¨æ¸…ç†7å¤©å‰çš„æ—¥å¿—å’Œæ•°æ®æ–‡ä»¶ã€‚

å¦‚éœ€ä¿®æ”¹ä¿ç•™å¤©æ•°,ç¼–è¾‘è„šæœ¬ä¸­çš„è¿™ä¸€è¡Œ:

```bash
find "$SCRIPT_DIR/logs" -name "crawler_*.log" -mtime +7 -delete
# å°† +7 æ”¹ä¸ºå…¶ä»–å¤©æ•°,å¦‚ +30 è¡¨ç¤ºä¿ç•™30å¤©
```

## ğŸ”’ å®‰å…¨å»ºè®®

### 1. ä¿æŠ¤é…ç½®æ–‡ä»¶

```bash
# è®¾ç½®é…ç½®æ–‡ä»¶æƒé™,åªæœ‰æ‰€æœ‰è€…å¯è¯»å†™
chmod 600 config.env

# ç¡®ä¿ä¸ä¼šè¢«Gitè·Ÿè¸ª
echo "config.env" >> .gitignore
```

### 2. ä½¿ç”¨érootç”¨æˆ·

ä¸è¦ä½¿ç”¨rootç”¨æˆ·è¿è¡Œçˆ¬è™«:

```bash
# åˆ›å»ºä¸“ç”¨ç”¨æˆ·
sudo useradd -m -s /bin/bash crawler

# åˆ‡æ¢åˆ°è¯¥ç”¨æˆ·
sudo su - crawler

# åœ¨è¯¥ç”¨æˆ·ä¸‹éƒ¨ç½²é¡¹ç›®
```

### 3. å®šæœŸæ›´æ–°

```bash
# å®šæœŸæ‹‰å–æœ€æ–°ä»£ç 
cd tech-info-crawler
git pull origin main

# æ›´æ–°ä¾èµ–
pip install -r requirements.txt --upgrade
```

## ğŸ› æ•…éšœæ’æŸ¥

### é—®é¢˜1: è„šæœ¬æ²¡æœ‰æ‰§è¡Œ

**æ£€æŸ¥cronæ˜¯å¦è¿è¡Œ**:
```bash
sudo systemctl status cron  # Ubuntu/Debian
sudo systemctl status crond  # CentOS/RHEL
```

**æ£€æŸ¥è„šæœ¬æƒé™**:
```bash
ls -l run_daily.sh
# åº”è¯¥æ˜¾ç¤º -rwxr-xr-x (å¯æ‰§è¡Œæƒé™)
```

**æ£€æŸ¥cronæ—¥å¿—**:
```bash
grep CRON /var/log/syslog  # Ubuntu/Debian
```

### é—®é¢˜2: Discordæ¨é€å¤±è´¥

**æ£€æŸ¥ç½‘ç»œè¿æ¥**:
```bash
curl -I https://discord.com
```

**æ£€æŸ¥Webhook URL**:
```bash
# æ‰‹åŠ¨æµ‹è¯•Webhook
curl -X POST "YOUR_WEBHOOK_URL" \
  -H "Content-Type: application/json" \
  -d '{"content": "æµ‹è¯•æ¶ˆæ¯"}'
```

### é—®é¢˜3: AIå¤„ç†å¤±è´¥

**æ£€æŸ¥OpenAI APIå¯†é’¥**:
```bash
# æµ‹è¯•APIå¯†é’¥
curl https://api.openai.com/v1/models \
  -H "Authorization: Bearer $OPENAI_API_KEY"
```

**æ£€æŸ¥APIä½™é¢**:
è®¿é—® https://platform.openai.com/account/usage

### é—®é¢˜4: Pythonä¾èµ–é—®é¢˜

**é‡æ–°å®‰è£…ä¾èµ–**:
```bash
pip3 install -r requirements.txt --force-reinstall
```

**æ£€æŸ¥Pythonç‰ˆæœ¬**:
```bash
python3 --version
# åº”è¯¥æ˜¯ 3.10 æˆ–æ›´é«˜
```

## ğŸ“Š ç›‘æ§å’Œå‘Šè­¦

### ä½¿ç”¨systemdæœåŠ¡(å¯é€‰)

åˆ›å»ºsystemdæœåŠ¡æ–‡ä»¶:

```bash
sudo nano /etc/systemd/system/tech-crawler.service
```

å†…å®¹:
```ini
[Unit]
Description=Tech Info Crawler Daily Job
After=network.target

[Service]
Type=oneshot
User=your-username
WorkingDirectory=/path/to/tech-info-crawler
ExecStart=/path/to/tech-info-crawler/run_daily.sh

[Install]
WantedBy=multi-user.target
```

é…ç½®å®šæ—¶å™¨:
```bash
sudo nano /etc/systemd/system/tech-crawler.timer
```

å†…å®¹:
```ini
[Unit]
Description=Run Tech Crawler Daily at 9 AM

[Timer]
OnCalendar=09:00
Persistent=true

[Install]
WantedBy=timers.target
```

å¯ç”¨:
```bash
sudo systemctl daemon-reload
sudo systemctl enable tech-crawler.timer
sudo systemctl start tech-crawler.timer
```

## ğŸ”„ æ›´æ–°å’Œç»´æŠ¤

### æ›´æ–°ä»£ç 

```bash
cd tech-info-crawler
git pull origin main
pip install -r requirements.txt --upgrade
```

### å¤‡ä»½é…ç½®

```bash
# å¤‡ä»½é…ç½®æ–‡ä»¶
cp config.env config.env.backup

# å¤‡ä»½å†å²è®°å½•
cp crawled_history.json crawled_history.json.backup
```

### æ¸…ç†æ•°æ®

```bash
# æ¸…ç†æ‰€æœ‰ä¸´æ—¶æ•°æ®(ä¿ç•™å†å²è®°å½•)
rm -f crawled_data_*.json processed_data_*.json daily_report_*.md

# æ¸…ç†æ‰€æœ‰æ—¥å¿—
rm -f logs/*.log
```

## ğŸ“ è·å–å¸®åŠ©

å¦‚æœé‡åˆ°é—®é¢˜:
1. æŸ¥çœ‹æ—¥å¿—æ–‡ä»¶: `logs/crawler_YYYYMMDD.log`
2. æŸ¥çœ‹æœ¬æ–‡æ¡£çš„æ•…éšœæ’æŸ¥éƒ¨åˆ†
3. åœ¨GitHubä»“åº“æäº¤Issue: https://github.com/Xiangyu-Li97/tech-info-crawler/issues

---

**ç¥æ‚¨éƒ¨ç½²é¡ºåˆ©!** ğŸ‰
