# 优质科技信息爬取系统 - 完整技术方案与实施指南

**作者**: Manus AI  
**日期**: 2026-01-09  
**版本**: 1.0

---

## 一、项目背景与目标

在信息爆炸的时代,高质量的科技资讯分散在全球各大媒体平台和专业网站中。对于关注 **AI 人工智能**、**Biotech 生物科技**、**硅谷创业** 和 **一级市场投资** 的从业者和研究者而言,手动追踪和筛选这些信息既耗时又低效。本项目旨在构建一个自动化的信息聚合系统,能够持续从多个权威信息源抓取内容,并通过智能化的处理流程对内容进行去重、分类和质量评估,最终为用户提供结构化、高质量的科技资讯数据。

系统的核心目标包括:

- **全面覆盖**: 聚合来自 TechCrunch、MIT Technology Review、Wired 等顶级科技媒体的最新资讯。
- **智能过滤**: 自动识别和标记文章的主题领域,并根据多维度指标评估内容质量。
- **自动化运行**: 通过定时任务实现无人值守的持续抓取和数据更新。
- **易于扩展**: 模块化的设计使得添加新信息源、调整分类规则和集成外部服务(如 LLM)变得简单。

## 二、系统架构设计

### 2.1 整体架构

系统采用 **分层模块化架构**,主要包含以下四个核心层次:

| 层次 | 模块 | 功能描述 |
|------|------|----------|
| **数据采集层** | `crawler.py` | 从多个 RSS 订阅源抓取原始文章数据 |
| **数据处理层** | `data_processor.py` | 对原始数据进行去重、分类和质量评分 |
| **调度与监控层** | `scheduler.py` | 定时触发爬取和处理任务,记录执行日志 |
| **数据展示层** | `viewer.py` | 提供交互式查看器和报告生成功能 |

这种分层设计使得每个模块职责清晰,便于独立开发、测试和维护。用户可以根据需求灵活组合这些模块,例如仅运行爬虫获取原始数据,或仅使用查看器分析已有数据。

### 2.2 数据流

系统的数据流程如下:

1. **采集阶段**: `crawler.py` 从配置的 RSS 源抓取文章元数据(标题、链接、摘要、发布时间等),并将原始数据保存为 JSON 文件。
2. **处理阶段**: `data_processor.py` 读取原始 JSON 文件,执行去重、关键词匹配分类和质量评分,输出处理后的 JSON 文件。
3. **存储阶段**: 处理后的数据以时间戳命名的 JSON 文件形式存储在本地,便于历史数据追溯和增量分析。
4. **展示阶段**: `viewer.py` 加载最新的处理数据,提供统计分析、分类筛选和 Markdown 报告导出功能。

### 2.3 技术选型

系统基于 **Python 3.10+** 构建,主要依赖以下第三方库:

| 库名 | 用途 | 选型理由 |
|------|------|----------|
| `feedparser` | RSS 订阅解析 | 成熟稳定,支持多种 RSS 和 Atom 格式 |
| `requests` | HTTP 请求 | 轻量高效,适合简单的网页抓取 |
| `beautifulsoup4` | HTML 解析 | 灵活易用,适合提取网页中的结构化数据 |
| `scrapy` | 分布式爬虫框架 | 可扩展性强,适合未来大规模抓取需求 |
| `playwright` | 浏览器自动化 | 支持 JavaScript 渲染,可绕过部分反爬虫机制 |
| `apscheduler` | 定时任务调度 | 轻量级,支持灵活的时间间隔和 cron 表达式 |

## 三、核心功能实现

### 3.1 数据采集 (crawler.py)

数据采集模块负责从预定义的 RSS 订阅源获取最新文章。当前版本主要使用 `feedparser` 库解析 RSS feed,这种方式对服务器友好且不易触发反爬虫机制。

**核心逻辑**:

- 遍历 `RSS_FEEDS` 字典中的每个信息源。
- 使用 `feedparser.parse()` 解析 RSS feed,提取文章的标题、链接、发布时间和摘要。
- 将提取的数据组织为统一的字典格式,并添加 `source` 和 `crawled_at` 字段以标识来源和抓取时间。
- 将所有文章数据保存为一个 JSON 文件,文件名包含时间戳以区分不同批次的抓取结果。

**当前支持的信息源**:

- **TechCrunch**: 全球领先的科技创业媒体,覆盖 AI、创业、投资等多个领域。
- **MIT Technology Review**: 麻省理工学院旗下的权威科技评论杂志,专注于前沿技术分析。
- **Wired**: 知名科技文化杂志,报道范围涵盖科技、商业和文化交叉领域。

**扩展性**: 用户可以通过编辑 `RSS_FEEDS` 字典轻松添加新的信息源。对于不提供 RSS 的网站或需要处理 JavaScript 渲染的页面,可以集成 `playwright` 或 `scrapy` 进行深度抓取(已在代码中预留接口)。

### 3.2 数据处理 (data_processor.py)

数据处理模块是系统的核心,负责将原始的、杂乱的文章数据转化为结构化、高质量的信息资产。

**主要功能**:

#### 3.2.1 内容去重

通过计算文章标题和链接的 MD5 哈希值,识别并移除重复的文章。这种方法简单高效,能够有效避免因多次抓取或多源重复导致的数据冗余。

#### 3.2.2 自动分类

基于 **关键词匹配** 的方法,系统内置了四大领域的关键词词典:

- **AI**: `ai`, `machine learning`, `deep learning`, `chatgpt`, `llm`, `computer vision`, `nlp` 等。
- **Biotech**: `biotech`, `pharma`, `drug`, `gene`, `crispr`, `clinical trial`, `vaccine` 等。
- **Startup**: `startup`, `founder`, `entrepreneur`, `silicon valley`, `y combinator` 等。
- **VC**: `venture capital`, `funding`, `investment`, `series a`, `ipo`, `unicorn` 等。

每篇文章可以同时归属于多个类别,这种多标签分类方式更符合科技资讯的跨领域特性。对于无法匹配任何关键词的文章,会被标记为 `General` 类别。

#### 3.2.3 质量评分

系统基于以下维度为每篇文章计算质量分:

- **信息源权威性**: MIT Technology Review 和 Fierce Biotech 等权威媒体获得更高的基础分。
- **标题长度**: 过短或过长的标题会被扣分,30-100 字符的标题被认为是最优的。
- **摘要丰富度**: 摘要长度超过 200 字符的文章会获得额外加分,因为这通常意味着内容更详实。
- **分类明确性**: 能够明确归类到特定领域的文章会获得额外加分。

质量分的范围通常在 10-30 分之间,分数越高表示文章的潜在价值越大。处理后的数据会按质量分降序排列,便于用户优先查看高质量内容。

### 3.3 定时调度 (scheduler.py)

调度模块基于 `APScheduler` 库实现,支持按固定时间间隔或 cron 表达式触发任务。当前版本配置为 **每2小时** 自动执行一次完整的爬取和处理流程。

**工作流程**:

1. 启动时立即执行一次爬取任务,确保用户能快速看到结果。
2. 调用 `crawler.py` 抓取最新数据。
3. 爬取成功后,自动调用 `data_processor.py` 对新数据进行处理。
4. 记录执行日志,包括开始时间、结束时间和执行状态。
5. 等待下一个调度周期,循环执行。

用户可以通过修改 `scheduler.py` 中的 `IntervalTrigger(hours=2)` 参数来调整执行频率,例如改为 `hours=1` 表示每小时执行一次。

### 3.4 数据查看与报告 (viewer.py)

查看器模块提供了一个交互式的命令行界面,用户可以通过菜单选择不同的操作:

- **数据统计**: 显示总文章数、按来源和分类的分布、质量评分的平均值和极值。
- **TOP 排行**: 展示质量评分最高的 10 篇文章,包括标题、来源、分类和链接。
- **分类筛选**: 按指定类别(AI、Biotech、Startup、VC)筛选文章,并显示该类别的 TOP 5。
- **报告生成**: 一键生成 Markdown 格式的完整报告,按类别组织内容,每个类别展示 TOP 20 文章。

生成的 Markdown 报告结构清晰,可以直接在支持 Markdown 的编辑器或平台中查看,也可以进一步转换为 PDF 或 HTML 格式进行分享。

## 四、部署与使用

### 4.1 环境准备

系统需要 Python 3.10 或更高版本。建议在 Linux 或 macOS 环境下运行,Windows 环境需要额外配置路径和权限。

### 4.2 安装依赖

在项目根目录下执行:

```bash
sudo pip3 install -r requirements.txt
```

该命令会自动安装 `feedparser`, `requests`, `beautifulsoup4`, `scrapy`, `playwright`, `apscheduler` 等所有必需的库。

### 4.3 快速开始

**手动执行一次爬取**:

```bash
cd /home/ubuntu/tech_info_crawler
python3 crawler.py
python3 data_processor.py
python3 viewer.py
```

**启动自动调度器**:

```bash
python3 scheduler.py
```

调度器会在前台运行并输出日志。如需后台运行,可以使用:

```bash
nohup python3 scheduler.py &> scheduler.log &
```

### 4.4 配置自定义信息源

编辑 `crawler.py` 文件,在 `RSS_FEEDS` 字典中添加新的 RSS 订阅源:

```python
RSS_FEEDS = {
    "TechCrunch": "http://feeds.feedburner.com/TechCrunch/",
    "MIT Technology Review": "https://www.technologyreview.com/feed/",
    "Wired": "https://www.wired.com/feed/rss",
    "Your New Source": "https://example.com/feed"
}
```

保存后,下次运行爬虫时就会自动包含新源。

## 五、系统优势与创新点

### 5.1 模块化设计

每个功能模块独立运行,用户可以根据需求灵活组合。例如,可以仅使用爬虫模块获取原始数据,然后用自己的脚本进行分析;也可以直接使用查看器模块分析他人提供的数据文件。

### 5.2 智能分类与评分

基于关键词的多标签分类方法简单高效,且易于扩展。质量评分机制综合考虑了信息源权威性、内容丰富度和分类明确性,能够有效筛选出高价值文章。未来可以进一步集成大语言模型(如 GPT-4)进行更精准的语义分析和质量评估。

### 5.3 轻量级与易部署

系统不依赖数据库或复杂的中间件,所有数据以 JSON 文件形式存储,部署和迁移非常简单。对于小规模应用(每天数百篇文章),这种方式完全能够满足需求。

### 5.4 可扩展性

系统预留了多个扩展接口:

- **爬虫层**: 可以集成 `playwright` 或 `scrapy` 以支持更复杂的网站抓取。
- **处理层**: 可以调用 OpenAI API 或本地 LLM 进行内容摘要、情感分析和主题建模。
- **存储层**: 可以将 JSON 文件迁移到 SQLite、PostgreSQL 或 Elasticsearch,以支持更复杂的查询和全文搜索。
- **展示层**: 可以基于 Flask 或 FastAPI 开发 Web 界面,提供更友好的用户体验。

## 六、未来发展方向

### 6.1 增强反爬虫能力

部分高价值网站(如 Fierce Biotech、PitchBook)采用了 Cloudflare 等反爬虫保护机制,当前版本的简单 RSS 解析无法绕过这些限制。未来可以集成 `playwright` 进行浏览器自动化,模拟真实用户行为以获取内容。

### 6.2 集成大语言模型

利用 GPT-4、Claude 等大语言模型进行:

- **智能摘要**: 为较长的文章生成简洁的摘要,提高阅读效率。
- **质量评估**: 基于语义理解判断文章是否包含实质性信息、数据支撑和独特见解。
- **主题提取**: 自动识别文章的核心主题和关键实体(如公司名、人名、技术名词)。

### 6.3 数据库与全文搜索

将数据存储从 JSON 文件迁移到关系型数据库(如 PostgreSQL)或搜索引擎(如 Elasticsearch),以支持:

- 复杂的多条件查询(如"查找最近一周内关于 AI 的所有文章")。
- 全文搜索和相关性排序。
- 历史数据的长期归档和趋势分析。

### 6.4 Web 界面与 API

开发一个基于 React 或 Vue 的 Web 前端,提供:

- 可视化的数据统计图表(如每日新增文章数、各类别占比等)。
- 交互式的文章浏览和筛选功能。
- RESTful API 接口,供第三方应用调用。

### 6.5 多语言支持

当前系统主要针对英文内容,未来可以扩展到中文、日文等其他语言的科技资讯,并开发多语言的关键词词典和分类模型。

## 七、总结

本项目提供了一个完整的、可立即投入使用的优质科技信息爬取工具。通过模块化的设计和智能化的数据处理流程,系统能够高效地从全球顶级科技媒体聚合资讯,并为用户提供结构化、高质量的数据。无论是个人研究者、创业者还是投资机构,都可以利用这个工具快速掌握 AI、Biotech、Startup 和 VC 领域的最新动态。

系统的代码简洁易懂,文档完善,且预留了丰富的扩展接口,用户可以根据自己的需求进行定制和优化。随着技术的发展和需求的变化,系统可以逐步演进为一个功能更强大、覆盖面更广的智能信息聚合平台。

---

**项目地址**: `/home/ubuntu/tech_info_crawler/`  
**技术支持**: 如有问题或建议,请参考 `README.md` 或联系开发者。
